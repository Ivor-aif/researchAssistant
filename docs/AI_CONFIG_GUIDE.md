# AI API 配置功能使用指南

## 功能概述

AI API 配置功能允许用户同时配置多个AI服务提供商，为不同的任务选择最适合的AI模型。系统支持设置主配置用于简单任务，并为特定功能配置专门的AI服务。

## 主要特性

### 🎯 多AI提供商支持
- **OpenAI**: GPT-3.5, GPT-4, GPT-4 Turbo等
- **Anthropic**: Claude-3 Haiku, Sonnet, Opus等
- **Google**: Gemini Pro, Gemini Pro Vision等
- **百度**: 文心一言系列
- **阿里云**: 通义千问系列
- **腾讯**: 混元大模型
- **智谱**: ChatGLM系列
- **月之暗面**: Kimi Chat
- **自定义**: 支持任何兼容OpenAI API的服务

### 🔧 灵活的配置管理
- **配置标题**: 为每个配置设置易记的名称
- **使用场景**: 为不同任务指定专用配置
- **主配置**: 设置默认使用的AI配置
- **提示词模板**: 为每个配置设置专用的系统和默认提示词
- **高级参数**: 精细调节温度、最大token数等参数

### 🎨 智能任务分配
系统会根据任务类型自动选择最适合的AI配置：
- **创新点分析**: 使用擅长分析的模型
- **论文推荐**: 使用理解能力强的模型
- **研究分析**: 使用逻辑推理能力强的模型
- **聊天对话**: 使用对话能力强的模型
- **提示词生成**: 使用主配置或指定配置

## 使用步骤

### 1. 访问AI配置页面
在侧边栏菜单中点击「AI配置」进入配置管理页面。

### 2. 添加AI配置

#### 基本信息
1. 点击「添加AI配置」按钮
2. 填写配置名称（如："GPT-4 创新分析"）
3. 选择AI提供商类型
4. 输入API密钥
5. 选择或输入模型名称

#### 高级设置
- **API端点**: 自定义API服务地址（可选）
- **系统提示词**: 设置AI的角色和行为
- **默认提示词**: 设置默认的用户输入模板
- **使用场景**: 选择此配置适用的功能模块
- **高级参数**: 调节温度、最大token数等

### 3. 设置主配置
- 在配置列表中点击星形图标设置主配置
- 主配置将用于提示词生成等简单任务
- 每个用户只能有一个主配置

### 4. 测试配置
- 点击测试按钮验证配置是否正常工作
- 输入测试提示词查看AI响应
- 系统会显示响应时间和结果

### 5. 生成AI提示词
使用内置的提示词生成器：
1. 点击「生成提示词」按钮
2. 输入关键词（用逗号分隔）
3. 选择任务类型
4. 提供上下文信息（可选）
5. 选择使用的AI配置（可选，默认使用主配置）
6. 点击生成并复制结果

## 配置示例

### 示例1：GPT-4 创新分析配置
```
配置名称: GPT-4 创新分析
AI提供商: OpenAI
API密钥: sk-xxx...
模型名称: gpt-4
系统提示词: 你是一个专业的科研创新分析专家，擅长识别和分析学术论文中的创新点。
使用场景: ✅ 创新点分析
温度: 0.3
最大Token: 4000
```

### 示例2：Claude-3 论文推荐配置
```
配置名称: Claude-3 论文推荐
AI提供商: Anthropic
API密钥: sk-ant-xxx...
模型名称: claude-3-sonnet-20240229
系统提示词: 你是一个学术论文推荐专家，能够根据用户的研究兴趣推荐相关的高质量论文。
使用场景: ✅ 论文推荐
温度: 0.5
最大Token: 8000
```

### 示例3：通义千问聊天配置
```
配置名称: 通义千问聊天助手
AI提供商: 阿里云
API密钥: sk-xxx...
模型名称: qwen-turbo
API端点: https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation
系统提示词: 你是一个友好的AI助手，能够帮助用户解答各种问题。
使用场景: ✅ 聊天对话
温度: 0.7
最大Token: 2000
```

## 最佳实践

### 🎯 任务专用配置
- 为不同任务创建专门的配置
- 根据模型特长分配任务（如GPT-4用于分析，Claude用于写作）
- 调整温度参数：分析任务用低温度(0.1-0.3)，创作任务用高温度(0.7-0.9)

### 🔐 安全管理
- 定期更换API密钥
- 不要在配置名称中包含敏感信息
- 及时禁用不再使用的配置

### 💰 成本控制
- 为不同配置设置合适的最大token数
- 优先使用成本较低的模型处理简单任务
- 监控API使用量，避免超出预算

### 🚀 性能优化
- 为快速响应的任务选择延迟较低的模型
- 根据任务复杂度选择合适的模型规模
- 使用缓存减少重复请求

## 故障排除

### 常见问题

**Q: API密钥无效**
- 检查密钥格式是否正确
- 确认密钥是否已过期
- 验证API端点是否正确

**Q: 模型不可用**
- 检查模型名称是否正确
- 确认账户是否有权限使用该模型
- 尝试使用其他可用模型

**Q: 响应超时**
- 检查网络连接
- 减少最大token数
- 尝试使用其他API端点

**Q: 配置测试失败**
- 检查所有必填字段是否已填写
- 验证API密钥和端点
- 查看错误信息进行针对性修复

### 调试技巧
- 使用测试功能验证配置
- 查看浏览器开发者工具的网络请求
- 检查后端日志获取详细错误信息

## API参考

### 前端服务
```typescript
// 获取用户所有配置
AIConfigService.getUserConfigs()

// 创建新配置
AIConfigService.createConfig(configData)

// 更新配置
AIConfigService.updateConfig(configId, updateData)

// 删除配置
AIConfigService.deleteConfig(configId)

// 设置主配置
AIConfigService.setPrimaryConfig(configId)

// 测试配置
AIConfigService.testConfig(configId, prompt)

// 生成提示词
AIConfigService.generatePrompt({
  keywords: ['机器学习', '深度学习'],
  task_type: 'innovation_analysis',
  context: '研究神经网络优化方法',
  config_id: 1
})
```

### 后端API
```
GET    /ai-config              # 获取用户配置列表
POST   /ai-config              # 创建新配置
GET    /ai-config/{id}         # 获取指定配置
PUT    /ai-config/{id}         # 更新配置
DELETE /ai-config/{id}         # 删除配置
POST   /ai-config/{id}/set-primary    # 设置主配置
POST   /ai-config/{id}/test           # 测试配置
POST   /ai-config/generate-prompt     # 生成提示词
GET    /ai-config/for-task/{task}     # 获取任务配置
GET    /ai-config/primary             # 获取主配置
```

## 更新日志

### v1.0.0 (2024-01-XX)
- ✨ 初始版本发布
- 🎯 支持多AI提供商配置
- 🔧 提供完整的配置管理界面
- 🚀 实现智能任务分配
- 💡 内置提示词生成器
- 🧪 配置测试功能

---

如有问题或建议，请联系开发团队或提交Issue。